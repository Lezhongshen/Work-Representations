{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python refresher: classes, methods, attributes.\n",
    "\n",
    "1. Create Python classes `professor` and `student`. The professor should have the attribute `courses_taught`, and the student - `courses_enrolled` (list of strings). Both of them should also have a `name` attribute (string), which is set when a class instance is initialized.\n",
    "2. Implement the methods to update the list of courses for the student and the professor classes.\n",
    "3. Initialize an instance of a student class named \"Jane Doe\", and a professor named \"Mary Smith\".\n",
    "4. Use the methods you implemented to get the student enrolled in 3 courses of your choosing, and the professor - teaching 2 other courses.\n",
    "5. Check whether the student is enrolled in any courses that the professor is teaching?\n",
    "\n",
    "Python refresher: [classes](https://www.pythontutorial.net/python-oop/python-class/), [attributes](https://www.pythontutorial.net/python-oop/python-class-attributes/), [methods](https://www.tutorialspoint.com/difference-between-method-and-function-in-python), [.self and __init__](https://micropyramid.com/blog/understand-self-and-__init__-method-in-python-class/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic operations with PyTorch\n",
    "1. Create two random Torch tensors of the size 4 x 6 and 6 x 8. [Multiply](https://pytorch.org/docs/stable/generated/torch.matmul.html) these tensors.\n",
    "2. Create 2 numpy arrays with shapes 4 x 3 and 7 x 3. Turn them into Torch tensors and [concatenate](https://pytorch.org/docs/stable/generated/torch.cat.html) them. Confirm that the shape is correct and turn the result back into a numpy array.\n",
    "3. Return the concatenated tensor to torch. Find which device it is in.\n",
    "\n",
    "Basic tensor operations tutorial: [link](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "If you're not familiar with numpy - it's a very popular library for data science, consider catching up with this [tutorial](https://numpy.org/doc/stable/user/quickstart.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the PyTorch model\n",
    "\n",
    "1. Finish the definition of this model. The `__init__` method should contain one fully connected layer linear layer with ReLU activation function, and one output (aka \"logits\" layer.\n",
    "2. In the forward pass, the model should do the following:\n",
    " - compute the input values on the fully-connected layer\n",
    " - pass them through the [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation function\n",
    " - compute and return the logits in the output layer\n",
    "3. Try initializing and inspecting the model as a toy_model instance. It should have 4 features, hidden_size 8, and 3 classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exercise template\n",
    "class SimpleNN(torch.nn.Module):\n",
    "\n",
    "    # initializing the model with a certain number of input features\n",
    "    # output classes, and size of hidden layer(s)\n",
    "    def __init__(self, n_features, hidden_size, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # creating one fully connected layer fc1\n",
    "        # that applies a linear transformation to the incoming data: y=xA^T +b\n",
    "        self.fc1 =\n",
    "\n",
    "        # setting the ReLU activation function on the fully connected layer\n",
    "        self.fc1_activ =\n",
    "\n",
    "        # setting up the layer that will return the final values for prediction\n",
    "        # this is often called \"logits\", but this is not the statistical log-odds function\n",
    "        self.fc_logits =\n",
    "\n",
    "    # you have to define the forward() method which will specify the forward propagation:\n",
    "    # how the input values get to the next layer(s)\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # compute the input values on the fully-connected layer\n",
    "        z1 =\n",
    "\n",
    "        # pass them through the activation function\n",
    "        z1_active =\n",
    "\n",
    "        # get the final values\n",
    "        logits =\n",
    "\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Computing the loss\n",
    "\n",
    "1. Instantiate the [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) loss function.\n",
    "2. Create two random tensors of the same shape: dummy_target and dummy_prediction. For this \"dry run\", let's pretend that these are the desired and actual outputs of our toy model. They need to be the size of the input layer.\n",
    "3. Compute the loss on the dummy tensors using the MSE loss function. Inspect the result.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backward propagation (single step)\n",
    "\n",
    "1. Instantiate the [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer with learning rate (`lr`) parameter set to 0.001.\n",
    "2. Zero out the current gradients of the optimizer with the [zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) method.\n",
    "3. Compute the gradients based on the loss function value by calling the `.backward()` method on the loss.\n",
    "4. Perform a single optimization step based on the computed gradients and inspect the loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exercise template\n",
    "optimizer = # initialize the adam optimizer here\n",
    "# zero out its gradients here\n",
    "# compute the gradients here\n",
    "optimizer.step()\n",
    "loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the Tweet_eval data and turning it to Torch tensors\n",
    "\n",
    "1. Load the tweet_eval data as usual (we will only need the train and validation sets)\n",
    "2. Vectorize the tweet texts with the TfIDF vectorizer from sklearn\n",
    "3. Convert this data to Torch tensors. Note that the original sklearn vector data is not numpy arrays but scipy matrices, which can be converted with `toarray()` method. Labels are lists, and so can be converted with `np.array(mylist)`.  You will also need to convert all the feature tensors to float type with `float()` method.\n",
    "\n",
    "If your computer is struggling with the conversion, simply reduce the amount of training data to a slice (e.g. first 10K examples).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Turning the data into Torch Datasets\n",
    "\n",
    "We're still not done with the data preparation! The canonical way to handle data in PyTorch is with the [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset) class. It is an abstract class representing any dataset used as input to a model. It is conveniently designed in a way that all the classes subclassing it would only have to override `__len__` and `__getitem__` methods. The goal of the `__getitem__` method is, given an index, to return the corresponding input data. There is an official PyTorch [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "You are provided with the skeleton code for the dataset class for our training data.\n",
    "\n",
    "1. Fill in the parts that provide the torch tensors corresponding to the vector and label data. Luckily, you have just done that in the previous step!\n",
    "2. Create the same kind of class for the validation data.\n",
    "3. Instantiate both classes and load them using `torch.utils.data.DataLoader`, with `batch_size` 64.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exercise template cell\n",
    "class TweetEvalTrain(torch.utils.data.Dataset):\n",
    "    # define how you're getting the data in your X and y attributes. They can be loaded from csv file,\n",
    "    # from some other resource, etc.\n",
    "    def __init__(self):\n",
    "        self.X = # tensor corresponding to the tfidf vectors for the train data\n",
    "        self.y = # tensor corresponding to the labels for the train data\n",
    "\n",
    "    # this method implements retrieval of a datapoint by index\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    # a helper to check the size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TweetEvalVal(torch.utils.data.Dataset):\n",
    "    pass #implement this class using the above as a template"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train = # instantiate your train data class\n",
    "train_loader = #load it using batch size 64\n",
    "\n",
    "# do the same for validation data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's train our neural network!\n",
    "\n",
    "1. Create an instance of our SimpleNN model using hidden_size 100. The input feature size should correspond to the size of tfidf vectors. We still have 3-class classification.\n",
    "2. Like before, set up the loss function ([CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) and [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate 0.001\n",
    "3. Complete and run the provided code for training the model across 5 epochs. Is your loss going down?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# skeleton code for step (3)\n",
    "for epoch in range(num_epochs):\n",
    "    losses = [] # storing the loss values\n",
    "    for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        # zeroing the gradients that are stored from the previous optimization step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = # compute the outputs\n",
    "        targets = torch.flatten(targets)\n",
    "        # compute the loss here\n",
    "\n",
    "        # back-propagate\n",
    "\n",
    "        # perform the optimization step\n",
    "        losses.append(loss.item())\n",
    "    print(f'Epoch {epoch}: loss {np.mean(losses)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the trained model\n",
    "\n",
    "Complete the following code to evaluate the model:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (459789159.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [160]\u001B[0;36m\u001B[0m\n\u001B[0;31m    outputs = # compute model outputs\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# skeleton code\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad(): #this is evaluation, so we don't need to do backpropagation anymore\n",
    "    for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "        outputs = # compute model outputs\n",
    "        # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "        vals, indices = torch.max(outputs, 1)\n",
    "        # accumulating the predictions\n",
    "        predictions += indices.tolist()\n",
    "\n",
    "# compute accuracy on the predicted and target values with sklearn accuracy_score.\n",
    "# Use the original list of validation labels loaded from the tweet_eval dataset\n",
    "acc =\n",
    "print(f'Model accuracy: {acc}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced, optional\n",
    "\n",
    "If you're done with the above, try to write the same kind of simple neural net model and its training loop from scratch, without looking at the skeleton code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}