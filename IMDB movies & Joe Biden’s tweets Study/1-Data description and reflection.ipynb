{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736f4d88",
   "metadata": {},
   "source": [
    "# Data description and reflection\n",
    "\n",
    "I use two data sources, IMDB movie data and twitter data, to make two different dataframes and conduct relevant analysis. \n",
    "\n",
    "### **1. IMDB**\n",
    "\n",
    "**(1) Data** \n",
    "\n",
    "I choose 500 movies that are shown in 2019 and have the highest popularity to do the relevant study.\n",
    "I collect these following data from IMDB website: 1) movie name; 2) movie length; 3) genre; 4) 10-scale rating; 5) 100-scale metascore; 6) vote number; 7) US/Canada box office gross; 8) World-wide box office gross; 9) opening week box office gross; 10) movie budget. After I collect these data, I add two variables based on these data. \n",
    "\n",
    "**(2) Data Processing**\n",
    "\n",
    "- Data extraction\n",
    "\n",
    "I use web scraping through python to obtain the data. First, I find out the web pages on IMDB that contains all movies in 2019 and sort the movies by popularity. I observe the web page links and find out the mode of links of each web page. Then I extract all the links for top 500 popular movies and store them into a list. Next, I use requests and BeautifulSoup to get the information that I can directly find on these web pages including movie name, movie length, 10-scale rating, metascore, vote number, US/Canada box office and genre. However, world-wide box office, opening week box office and budget data are in the detailed information pages of the movies. Therefore, I extract all links of detailed information pages for each movie and get these three data from each detailed page. \n",
    "\n",
    "- Data cleaning\n",
    "\n",
    "I mostly use replace, string method and list comprehension to clean the useless information in the data. In the budget data, several movies’ budgets are calculated in currencies other than US dollars, so I find out the exchange rate in 2019 of each of these currencies and exchange them into US dollars. I also convert all numbers into float. Finally, I turn all these data into a dataframe. To reduce the quantity dimension, I turn the units of all box office data into million.\n",
    "\n",
    "- Data expansion\n",
    "\n",
    "Later I add two variables to the dataframe. One is a binary variable, high/low score, in which ‘1’ represents 10-scale rating is equal to or over 7 and ‘0’ represents 10-scale rating is lower than 7. Another is a non-binary categorical variable, movie length type, in which ‘long’ represents movies have length over 120 minutes, ‘normal’ represents movies have length between interval [90,120], ‘short’ represents movies have length lower than 90 minutes. \n",
    "\n",
    "**(3) Ethical problems**\n",
    "\n",
    "As the movies’ information are all public and I don’t collect any personal data like comments and personal rating, I don’t see any ethical problems in this study. However, if in the further study I am going to make analysis on people's comments on the movies, I should consider certain ethical dilemmas. Movie comments are very likely to be considered as private content because some comments may contain private emotions or opinions and when we collect their comments we will also collect the information of users. Therefore, if I collect these data I should consider problems like if people are aware that their movie comments are being collected and if I should ask for their consents of being collected data. \n",
    "\n",
    "\n",
    "### **2. Twitter**\n",
    "\n",
    "**(1) Data**\n",
    "\n",
    "I choose all Joe Biden’s tweets in 2021 to do the relevant study. I collect these following data from Joe Biden’s twitter account: 1) tweets text content; 2) tweets created time; 3) tweets like count; 4) tweets retweets count. After collecting these data, I add 8 extra variables based on these data.\n",
    "\n",
    "**(2) Data processing**\n",
    "\n",
    "- Data extraction\n",
    "\n",
    "I use twitter API tweepy to extract the data. First, I use Biden’s twitter screen name ‘POTUS’ to get the user timeline. As tweepy has restrictions on normal way to get user timeline, I use tweepy.Cursor to extract all Biden’s tweets. Then, I obtain tweets content, tweets created time, tweets like count and tweets retweet count in ._json of each tweet.\n",
    "\n",
    "- Data cleaning\n",
    "\n",
    "First, I transform all the texts into lowercase in order to imoprove the study efficiency. Then I turn all data into a dataframe. Next, I use pd.to_datatime to turn all tweets created time data into date time object. Then, I select tweets that were sent in 2021 based on date time and make a new dataframe. Finally, I remove tweets that have 0 like as these are Biden's retweets. To reduce the quantity dimension, I turn the units of like count and retweet count into thousand.\n",
    "\n",
    "- Data expansion\n",
    "\n",
    "*Word count*\n",
    "\n",
    "I make a dataframe for word count of Biden’s tweets content. The word count includes: 1) top 100 frequent words in all the texts without sorting; 2) top 100 and last 100 frequent words sorted by like count; 3) top 100 and last 100 frequent words sorted by retweet count. While conducting word count, I remove the stop words and meaningless symbols. To make the dataframe more intuitive, I take top 100 and last 100 words from the count to make the dataframe.\n",
    "\n",
    "*More variables*\n",
    "\n",
    "Based on the text content and my study interest, I add 8 more variables to the dataframe, including 7 binary variables and 1 non-binary categorical variable. First, I make a variable for whether a tweet includes a link, with 1 representing link exists and 0 representing link doesn’t exist. Next, I choose 5 topics, Covid, economy, infrastructure law, climate and China to study. I make different lists of words that are related to each topic. These words come from my observation of the word count. Then I test the texts whether they contain these words and make binary variable in which 1 represents the tweet includes any word from one topic list and 0 means it doesn’t include. The non-binary categorical variable, tweets created quarter, is made based on the created time of tweets. Tweets created in the first quarter belongs to Q1 and so on for each quarter. Finally, I make a binary variable to represent whether the like count of the tweet exceeds 50 thousand. If so, then the value is 1, otherwise is 0.\n",
    "\n",
    "**(3) Ethical problems**\n",
    "\n",
    "When we use twitter API to extract users’ information, we should always consider about the ethical dilemma that is if users are aware that their data are being collected and if we should contact users for consent. However, in this study, the user I choose is a bit different. As the president of the United States of America, Joe Biden is spotlighted by the whole world and his public twitter account is no longer a private user account but an official public channel to communicate with people and express political information, on behalf of the president power and the party power. In my study, the information I extract such as Biden’s tweets content, like and retweet number all can be accounted to this public range, so I think the ethical concerns are mostly eliminated. Nevertheless, if in the further study I am going to extract users’ comments on Biden’s tweets and analyze, I will encounter noticeable ethical problems, as comments can be considered something private and people may not expect others to collect their comments as well as their personal information which will be revealed when we collect the comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
